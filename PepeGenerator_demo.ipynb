{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "58a0a3cceed94b229e6369846b9ef4cc": {
     "model_module": "@jupyter-widgets/output",
     "model_name": "OutputModel",
     "model_module_version": "1.0.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_3c150adbe7fb4708952c1e0023552a3c",
      "msg_id": "",
      "outputs": [
       {
        "output_type": "display_data",
        "data": {
         "text/plain": "  \u001B[37mGenerating 16 images\u001B[0m \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1000/1000\u001B[0m \u001B[33m0:04:28\u001B[0m \u001B[36m0:00:00\u001B[0m\n",
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Generating 16 images</span> <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">1000/1000</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:04:28</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n</pre>\n"
        },
        "metadata": {}
       }
      ]
     }
    },
    "3c150adbe7fb4708952c1e0023552a3c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare colab environment"
   ],
   "metadata": {
    "id": "6kHUawSGwLA5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f'Running in google colab: {IN_COLAB}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URxGGSLykzl7",
    "outputId": "6cd3e335-a1c9-487b-e67f-46af5337506d",
    "ExecuteTime": {
     "end_time": "2023-08-20T16:51:34.846898994Z",
     "start_time": "2023-08-20T16:51:34.846227868Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in google colab: False\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if IN_COLAB:\n",
    "    import gdown\n",
    "    # Prepare colab environment\n",
    "    !git clone https://github.com/Sleepon1805/PepeGeneration.git\n",
    "    print('Successfully cloned PepeGeneration repo.')\n",
    "\n",
    "    os.chdir(\"/content/PepeGeneration\")\n",
    "    print(f'Changed working directory to {os.getcwd()}')\n",
    "\n",
    "    !pip install --upgrade pip\n",
    "    !pip install -r requirements_demo.txt\n",
    "    print('Successfully installed all requirements.')\n",
    "\n",
    "    # Download checkpoint from shared Drive file\n",
    "    drive_link_id = '13byaG4vybYpgWvdYo9NScNm0bbKLxnQY'\n",
    "    gdown.download_folder(id=drive_link_id, quiet=False)\n",
    "    print('Successfully downloaded checkpoint folder.')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2s1SF41Wm5U1",
    "outputId": "4c3598f7-9767-4220-ffc1-de7f09c9a3e3",
    "ExecuteTime": {
     "end_time": "2023-08-20T16:51:34.847421691Z",
     "start_time": "2023-08-20T16:51:34.846725457Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load checkpoint and config"
   ],
   "metadata": {
    "id": "VKsvYaxMw3e1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from model.pepe_generator import PepeGenerator\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {DEVICE} device')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ydYrHljIV84",
    "outputId": "ed4c57fb-172c-46fd-978b-97ec0ec94510",
    "ExecuteTime": {
     "end_time": "2023-08-20T16:51:37.724479094Z",
     "start_time": "2023-08-20T16:51:34.847071159Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Specify path to model checkpoint and config here!!!\n",
    "ckpt_path = '/home/sleepon/repos/PepeGenerator/lightning_logs/celeba/version_11/checkpoints/epoch=07-fid_metric=1.83-val_loss=0.0254.ckpt'\n",
    "config_path = '/home/sleepon/repos/PepeGenerator/lightning_logs/celeba/version_11/config.pkl'\n",
    "\n",
    "assert os.path.exists(ckpt_path), 'Did not found path with model checkpoint. Check that checkpoint is downloaded and path is correct.'\n",
    "assert os.path.exists(config_path), 'Did not found path with model config. Check that config.pkl file is downloaded and path is correct.'\n",
    "print('Specified paths with model checkpoint and config exist.')\n",
    "print(f'Model checkpoint path: {ckpt_path}')\n",
    "print(f'Model config path: {ckpt_path}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3arQ-Lxwdwh",
    "outputId": "4df243da-ad6f-4d2d-8566-cbe6ecf73c66",
    "ExecuteTime": {
     "end_time": "2023-08-20T16:51:37.725259517Z",
     "start_time": "2023-08-20T16:51:37.722232012Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified paths with model checkpoint and config exist.\n",
      "Model checkpoint path: /home/sleepon/repos/PepeGenerator/lightning_logs/celeba/version_11/checkpoints/epoch=07-fid_metric=1.83-val_loss=0.0254.ckpt\n",
      "Model config path: /home/sleepon/repos/PepeGenerator/lightning_logs/celeba/version_11/checkpoints/epoch=07-fid_metric=1.83-val_loss=0.0254.ckpt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# load config\n",
    "with open(config_path, 'rb') as config_file:\n",
    "    config = pickle.load(config_file)\n",
    "pprint(config)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPCeewJXMNOv",
    "outputId": "78c492de-d6aa-44ac-8835-05db46859bdb",
    "ExecuteTime": {
     "end_time": "2023-08-20T16:51:37.737722280Z",
     "start_time": "2023-08-20T16:51:37.725465135Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(git_hash='7616ce5+',\n",
      "       sde_training=False,\n",
      "       batch_size=64,\n",
      "       image_size=64,\n",
      "       lr=0.0001,\n",
      "       scheduler='MultiStepLR',\n",
      "       gradient_clip_algorithm='norm',\n",
      "       gradient_clip_val=0.5,\n",
      "       dataset_split=(0.8, 0.2),\n",
      "       dataset_name='celeba',\n",
      "       use_condition=False,\n",
      "       condition_size=40,\n",
      "       pretrained_ckpt='./lightning_logs/celeba/version_6/checkpoints/last.ckpt',\n",
      "       diffusion_steps=1000,\n",
      "       beta_min=0.0001,\n",
      "       beta_max=0.02,\n",
      "       init_channels=128,\n",
      "       channel_mult=(1, 2, 4, 4),\n",
      "       conv_resample=True,\n",
      "       num_heads=1,\n",
      "       dropout=0.3,\n",
      "       use_second_attention=True)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# load checkpoint\n",
    "model = PepeGenerator.load_from_checkpoint(ckpt_path, config=config, strict=True)\n",
    "model.eval(), model.freeze(), model.to(DEVICE)\n",
    "print('Loaded model')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Q50kYJZNnDc",
    "outputId": "4b5f7732-8550-4540-c0e7-bf6d77739db7",
    "ExecuteTime": {
     "end_time": "2023-08-20T16:51:38.660494266Z",
     "start_time": "2023-08-20T16:51:37.735245224Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup Sampler"
   ],
   "metadata": {
    "id": "if3GPaxTxWI2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from rich.progress import Progress, SpinnerColumn, TimeElapsedColumn, BarColumn, TextColumn, TimeRemainingColumn, MofNCompleteColumn\n",
    "\n",
    "from config import SamplingConfig\n",
    "from SDE_sampling.sde_samplers import PC_Sampler, ODE_Sampler\n",
    "\n",
    "def evaluate_model(sampling_config: SamplingConfig, grid_shape=(4, 4)):\n",
    "    # rich progress bar\n",
    "    progress = Progress(\n",
    "        SpinnerColumn(),\n",
    "        TextColumn(\"[progress.description]{task.description}\"),\n",
    "        BarColumn(),\n",
    "        MofNCompleteColumn(),\n",
    "        TimeElapsedColumn(),\n",
    "        TimeRemainingColumn()\n",
    "    )\n",
    "\n",
    "    # set up sampler\n",
    "    if sampling_config.sampler.lower() in ('ddpm', 'default'):\n",
    "        print('Using default DDPM Sampler as evaluation sampler.')\n",
    "    elif sampling_config.sampler.lower() == 'ode_solver':\n",
    "        print('Using ODE Solver as evaluation sampler.')\n",
    "        model.sampler = ODE_Sampler(config, sampling_config)\n",
    "    elif sampling_config.sampler.lower() == 'pc_sampler':\n",
    "        print('Using PC Sampler as evaluation sampler.')\n",
    "        model.sampler = PC_Sampler(config, sampling_config)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    model.sampler.to(DEVICE)\n",
    "\n",
    "    # create fake batch (actually needed only for shapes)\n",
    "    num_samples = grid_shape[0] * grid_shape[1]\n",
    "    fake_image_batch = torch.zeros((num_samples, 3, config.image_size, config.image_size))\n",
    "    fake_cond_batch = torch.ones(num_samples, config.condition_size)\n",
    "    fake_batch = (fake_image_batch, fake_cond_batch)\n",
    "\n",
    "    # generate images\n",
    "    with progress:\n",
    "        # [grid_shape[0] * grid_shape[1] x 3 x cfg.image_size x cfg.image_size]\n",
    "        gen_samples = model.generate_samples(fake_batch, progress=progress)\n",
    "    gen_images = model.sampler.generated_samples_to_images(gen_samples, grid_shape)\n",
    "\n",
    "    # show generated images\n",
    "    plt.imshow(gen_images)\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "9yoDSrVzyaaN",
    "ExecuteTime": {
     "end_time": "2023-08-20T16:51:38.810301236Z",
     "start_time": "2023-08-20T16:51:38.656099011Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Choose values you want to try out\n",
    "sampling_config = SamplingConfig(\n",
    "    sampler = 'pc_sampler',  # ddpm = default, pc_sampler, ode_solver\n",
    "    sde_name = 'VPSDE',  # VPSDE, subVPSDE, VESDE\n",
    "    beta_min = 0.1,  # VPSDE, subVPSDE param\n",
    "    beta_max = 20.,  # VPSDE, subVPSDE param\n",
    "    sigma_min = 0.01,  # VESDE param\n",
    "    sigma_max = 50.,  # VESDE param\n",
    "    num_scales = 1000,\n",
    "    predictor_name = 'euler_maruyama',  # none, ancestral_sampling, reverse_diffusion, euler_maruyama\n",
    "    corrector_name = 'langevin',  # none, langevin, ald\n",
    "    snr = 0.01,\n",
    "    num_corrector_steps = 1,\n",
    "    probability_flow = False,\n",
    "    denoise = False,\n",
    ")\n",
    "\n",
    "evaluate_model(sampling_config=sampling_config, grid_shape=(4, 4))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468,
     "referenced_widgets": [
      "58a0a3cceed94b229e6369846b9ef4cc",
      "3c150adbe7fb4708952c1e0023552a3c"
     ]
    },
    "id": "ZUOD5rojxeAj",
    "outputId": "b55ea959-dfec-4204-aac2-c80f668639ad",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-20T16:51:38.811501091Z"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d49d686d02e440b98407a3232d145fe8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "hQK96Rt1hTUq",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
